{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the training dataset:\n",
      "     value_eur  wage_eur  age  height_cm  weight_kg  club_jersey_number  \\\n",
      "0  181500000.0  230000.0   24        182         75                 7.0   \n",
      "1  185000000.0  340000.0   22        195         94                 9.0   \n",
      "2  103000000.0  350000.0   32        181         75                17.0   \n",
      "3   41000000.0   23000.0   36        169         67                10.0   \n",
      "4   51000000.0   95000.0   35        185         81                 9.0   \n",
      "\n",
      "   weak_foot  skill_moves  attacking_crossing  attacking_finishing  ...  \\\n",
      "0          4            5                  78                   94  ...   \n",
      "1          3            3                  47                   96  ...   \n",
      "2          5            4                  95                   85  ...   \n",
      "3          4            4                  83                   89  ...   \n",
      "4          4            4                  75                   91  ...   \n",
      "\n",
      "   body_type_Lean (170-185)  body_type_Lean (185+)  body_type_Normal (170-)  \\\n",
      "0                         0                      0                        0   \n",
      "1                         0                      0                        0   \n",
      "2                         0                      0                        0   \n",
      "3                         0                      0                        0   \n",
      "4                         0                      0                        0   \n",
      "\n",
      "   body_type_Normal (170-185)  body_type_Normal (185+)  \\\n",
      "0                           0                        0   \n",
      "1                           0                        0   \n",
      "2                           0                        0   \n",
      "3                           0                        0   \n",
      "4                           1                        0   \n",
      "\n",
      "   body_type_Stocky (170-)  body_type_Stocky (170-185)  \\\n",
      "0                        0                           0   \n",
      "1                        0                           0   \n",
      "2                        0                           0   \n",
      "3                        0                           0   \n",
      "4                        0                           0   \n",
      "\n",
      "   body_type_Stocky (185+)  body_type_Unique  first_position  \n",
      "0                        0                 1              14  \n",
      "1                        0                 1              14  \n",
      "2                        0                 1               4  \n",
      "3                        0                 1               3  \n",
      "4                        0                 0               3  \n",
      "\n",
      "[5 rows x 64 columns]\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Mean Squared Error: 13.030867579908676\n",
      "All tasks completed successfully. Models and graphs are saved in their respective directories.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, validation_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "graphs_dir = 'graphs_knn'\n",
    "models_dir = 'models_knn'\n",
    "os.makedirs(graphs_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'fifa_players_processed.csv'\n",
    "data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# List of columns to save for post-analysis\n",
    "post_analysis_columns = [\n",
    "    'player_id', 'overall', 'potential', 'club_team_id', 'league_name', \n",
    "    'nationality_name', 'st', 'lw', 'cf', 'rw', 'cam', 'lm', 'cm', 'rm', \n",
    "    'lwb', 'cdm', 'rwb', 'lb', 'cb', 'rb', 'gk', 'alternative_positions'\n",
    "]\n",
    "\n",
    "# Separate the post-analysis columns\n",
    "post_analysis_data = data[post_analysis_columns]\n",
    "\n",
    "# Drop the post-analysis columns from the main dataset\n",
    "data_cleaned = data.drop(columns=post_analysis_columns)\n",
    "\n",
    "# Ensure there are no NaNs in 'first_position'\n",
    "assert data_cleaned['first_position'].isnull().sum() == 0, \"There are NaNs in the 'first_position' column.\"\n",
    "\n",
    "# Encode the target feature 'first_position' as categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_labels = label_encoder.fit_transform(data_cleaned['first_position'])\n",
    "\n",
    "# Save the label encoder classes for later use in confusion matrix\n",
    "label_classes = label_encoder.classes_\n",
    "\n",
    "# Drop the original 'first_position' column from the cleaned data\n",
    "X = data_cleaned.drop(columns=['first_position'])\n",
    "\n",
    "# Combine the cleaned data with the target labels\n",
    "data_for_training = X.copy()\n",
    "data_for_training['first_position'] = y_labels\n",
    "\n",
    "# Save the post-analysis dataset to a CSV file\n",
    "post_analysis_file_path = 'fifa_post_analysis.csv'\n",
    "post_analysis_data.to_csv(post_analysis_file_path, index=False)\n",
    "\n",
    "# Save the training dataset to a CSV file\n",
    "training_file_path = 'fifa_training_data.csv'\n",
    "data_for_training.to_csv(training_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the training dataset\n",
    "print(\"\\nFirst few rows of the training dataset:\")\n",
    "print(data_for_training.head())\n",
    "\n",
    "# Load training data\n",
    "data_path = 'fifa_training_data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "X = data.drop('first_position', axis=1)\n",
    "y = data['first_position']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 10]\n",
    "}\n",
    "\n",
    "# Initialize the KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Save the best model\n",
    "best_model_path = os.path.join(models_dir, 'best_model_knn.joblib')\n",
    "joblib.dump(grid_search.best_estimator_, best_model_path)\n",
    "\n",
    "# Generate learning curves and validation curves for each combination of hyperparameters\n",
    "suffix_count = 1\n",
    "for params in grid_search.cv_results_['params']:\n",
    "    clf = KNeighborsClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Learning curve\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X_train, y_train, cv=5)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "    plt.title(f'Learning Curve for KNN (n_neighbors={params[\"n_neighbors\"]})')\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    learning_curve_path = os.path.join(graphs_dir, f'learning_curve_knn_{params[\"n_neighbors\"]}_suffix{suffix_count}.png')\n",
    "    plt.savefig(learning_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Validation curve for n_neighbors\n",
    "    param_range = param_grid['n_neighbors']\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        clf, X_train, y_train, param_name=\"n_neighbors\", param_range=param_range, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(param_range, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "    plt.plot(param_range, test_scores_mean, 'o-', color='g', label='Validation score')\n",
    "    plt.title(f'Validation Curve for KNN (n_neighbors={params[\"n_neighbors\"]})')\n",
    "    plt.xlabel('n_neighbors')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    validation_curve_path = os.path.join(graphs_dir, f'validation_curve_knn_{params[\"n_neighbors\"]}_suffix{suffix_count}.png')\n",
    "    plt.savefig(validation_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    suffix_count += 1\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_classes, yticklabels=label_classes)\n",
    "plt.title('Confusion Matrix for Best KNN Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "conf_matrix_path = os.path.join(graphs_dir, f'confusion_matrix_best_knn_suffix{suffix_count}.png')\n",
    "plt.savefig(conf_matrix_path)\n",
    "plt.close()\n",
    "\n",
    "# Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "print(\"All tasks completed successfully. Models and graphs are saved in their respective directories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix with percentages saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Your confusion matrix (assuming it is stored in conf_matrix)\n",
    "conf_matrix = np.array([\n",
    "    [118, 0, 6, 1, 84, 0, 5, 39, 10, 0, 2, 26, 2, 0, 21],\n",
    "    [0, 811, 38, 0, 14, 0, 30, 0, 0, 1, 26, 0, 0, 1, 1],\n",
    "    [3, 78, 188, 0, 129, 0, 11, 0, 0, 12, 0, 0, 0, 0, 1],\n",
    "    [15, 0, 0, 1, 7, 0, 1, 4, 2, 0, 1, 6, 1, 0, 13],\n",
    "    [53, 13, 113, 0, 451, 0, 20, 8, 1, 1, 5, 5, 1, 1, 3],\n",
    "    [0, 0, 0, 0, 0, 611, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [5, 36, 11, 0, 46, 0, 236, 3, 2, 10, 17, 3, 0, 0, 0],\n",
    "    [49, 2, 8, 1, 35, 0, 15, 79, 10, 3, 2, 34, 6, 0, 52],\n",
    "    [32, 0, 1, 0, 14, 0, 2, 25, 9, 0, 2, 20, 1, 0, 28],\n",
    "    [4, 3, 1, 0, 9, 0, 61, 0, 0, 1, 2, 2, 1, 0, 1],\n",
    "    [3, 40, 48, 0, 69, 0, 1, 4, 1, 0, 187, 1, 0, 5, 2],\n",
    "    [43, 0, 7, 3, 57, 0, 6, 52, 11, 1, 10, 54, 5, 0, 34],\n",
    "    [25, 0, 0, 0, 17, 0, 3, 32, 4, 0, 3, 27, 6, 0, 32],\n",
    "    [3, 11, 7, 0, 28, 0, 1, 1, 0, 1, 42, 0, 0, 3, 2],\n",
    "    [11, 2, 4, 0, 7, 0, 1, 27, 5, 0, 0, 16, 4, 0, 626]\n",
    "])\n",
    "\n",
    "# Class labels (assuming they are stored in label_classes)\n",
    "label_classes = ['CAM', 'CB', 'CDM', 'CF', 'CM', 'GK', 'LB', 'LM', 'LW', 'LWB', 'RB', 'RM', 'RW', 'RWB', 'ST']\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=label_classes, yticklabels=label_classes)\n",
    "plt.title('Confusion Matrix for Best KNN Model (Percentage)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "conf_matrix_path = os.path.join('graphs_knn', f'confusion_matrix_best_knn_percentage.png')\n",
    "plt.savefig(conf_matrix_path)\n",
    "plt.close()\n",
    "\n",
    "print(\"Confusion matrix with percentages saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
