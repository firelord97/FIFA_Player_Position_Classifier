{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the training dataset:\n",
      "     value_eur  wage_eur  age  height_cm  weight_kg  club_jersey_number  \\\n",
      "0  181500000.0  230000.0   24        182         75                 7.0   \n",
      "1  185000000.0  340000.0   22        195         94                 9.0   \n",
      "2  103000000.0  350000.0   32        181         75                17.0   \n",
      "3   41000000.0   23000.0   36        169         67                10.0   \n",
      "4   51000000.0   95000.0   35        185         81                 9.0   \n",
      "\n",
      "   weak_foot  skill_moves  attacking_crossing  attacking_finishing  ...  \\\n",
      "0          4            5                  78                   94  ...   \n",
      "1          3            3                  47                   96  ...   \n",
      "2          5            4                  95                   85  ...   \n",
      "3          4            4                  83                   89  ...   \n",
      "4          4            4                  75                   91  ...   \n",
      "\n",
      "   body_type_Lean (170-185)  body_type_Lean (185+)  body_type_Normal (170-)  \\\n",
      "0                         0                      0                        0   \n",
      "1                         0                      0                        0   \n",
      "2                         0                      0                        0   \n",
      "3                         0                      0                        0   \n",
      "4                         0                      0                        0   \n",
      "\n",
      "   body_type_Normal (170-185)  body_type_Normal (185+)  \\\n",
      "0                           0                        0   \n",
      "1                           0                        0   \n",
      "2                           0                        0   \n",
      "3                           0                        0   \n",
      "4                           1                        0   \n",
      "\n",
      "   body_type_Stocky (170-)  body_type_Stocky (170-185)  \\\n",
      "0                        0                           0   \n",
      "1                        0                           0   \n",
      "2                        0                           0   \n",
      "3                        0                           0   \n",
      "4                        0                           0   \n",
      "\n",
      "   body_type_Stocky (185+)  body_type_Unique  first_position  \n",
      "0                        0                 1              14  \n",
      "1                        0                 1              14  \n",
      "2                        0                 1               4  \n",
      "3                        0                 1               3  \n",
      "4                        0                 0               3  \n",
      "\n",
      "[5 rows x 64 columns]\n",
      "\n",
      "First few rows of the post-analysis dataset:\n",
      "   player_id  overall  potential  club_team_id          league_name  \\\n",
      "0     231747       91         94          73.0              Ligue 1   \n",
      "1     239085       91         94          10.0       Premier League   \n",
      "2     192985       91         91          10.0       Premier League   \n",
      "3     158023       90         90      112893.0  Major League Soccer   \n",
      "4     165153       90         90         607.0           Pro League   \n",
      "\n",
      "  nationality_name  st  lw  cf  rw  ...  cm  rm  lwb  cdm  rwb  lb  cb  rb  \\\n",
      "0           France  93  91  91  91  ...  84  92   71   66   71  66  57  66   \n",
      "1           Norway  93  82  86  82  ...  77  82   65   66   65  63  65  63   \n",
      "2          Belgium  86  87  88  87  ...  91  91   82   83   82  78  73  78   \n",
      "3        Argentina  88  90  89  90  ...  88  90   67   66   67  62  52  62   \n",
      "4           France  90  86  89  86  ...  85  89   67   67   67  63  58  63   \n",
      "\n",
      "   gk  alternative_positions  \n",
      "0  21                 ['LW']  \n",
      "1  22                     []  \n",
      "2  24                ['CAM']  \n",
      "3  22                ['CAM']  \n",
      "4  21                 ['ST']  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chinu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "Mean Squared Error: 8.109041095890412\n",
      "All tasks completed successfully. Models and graphs are saved in their respective directories.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import History, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "graphs_dir = 'graphs'\n",
    "models_dir = 'models'\n",
    "os.makedirs(graphs_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'fifa_players_processed.csv'\n",
    "data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# List of columns to save for post-analysis\n",
    "post_analysis_columns = [\n",
    "    'player_id', 'overall', 'potential', 'club_team_id', 'league_name', \n",
    "    'nationality_name', 'st', 'lw', 'cf', 'rw', 'cam', 'lm', 'cm', 'rm', \n",
    "    'lwb', 'cdm', 'rwb', 'lb', 'cb', 'rb', 'gk', 'alternative_positions'\n",
    "]\n",
    "\n",
    "# Separate the post-analysis columns\n",
    "post_analysis_data = data[post_analysis_columns]\n",
    "\n",
    "# Drop the post-analysis columns from the main dataset\n",
    "data_cleaned = data.drop(columns=post_analysis_columns)\n",
    "\n",
    "# Ensure there are no NaNs in 'first_position'\n",
    "assert data_cleaned['first_position'].isnull().sum() == 0, \"There are NaNs in the 'first_position' column before encoding.\"\n",
    "\n",
    "# Encode the target feature 'first_position' as categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_labels = label_encoder.fit_transform(data_cleaned['first_position'])\n",
    "\n",
    "# Save the label encoder classes for later use in confusion matrix\n",
    "label_classes = label_encoder.classes_\n",
    "\n",
    "# Drop the original 'first_position' column from the cleaned data\n",
    "X = data_cleaned.drop(columns=['first_position'])\n",
    "\n",
    "# Combine the cleaned data with the target labels\n",
    "data_for_training = X.copy()\n",
    "data_for_training['first_position'] = y_labels\n",
    "\n",
    "# Save the post-analysis dataset to a CSV file\n",
    "post_analysis_file_path = 'fifa_post_analysis.csv'\n",
    "post_analysis_data.to_csv(post_analysis_file_path, index=False)\n",
    "\n",
    "# Save the training dataset to a CSV file\n",
    "training_file_path = 'fifa_training_data.csv'\n",
    "data_for_training.to_csv(training_file_path, index=False)\n",
    "\n",
    "# Display the first few rows of the training dataset\n",
    "print(\"\\nFirst few rows of the training dataset:\")\n",
    "print(data_for_training.head())\n",
    "\n",
    "# Display the first few rows of the post-analysis dataset\n",
    "print(\"\\nFirst few rows of the post-analysis dataset:\")\n",
    "print(post_analysis_data.head())\n",
    "\n",
    "# Load training data\n",
    "data_path = 'fifa_training_data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "X = data.drop('first_position', axis=1)\n",
    "y = data['first_position']\n",
    "\n",
    "# Convert target variable to categorical\n",
    "y_categorical = to_categorical(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "def create_model(activation='relu', learning_rate=0.001, dropout_rate=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation=activation))\n",
    "    if dropout_rate:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    if dropout_rate:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Cross-validation settings\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "activation_functions = ['relu', 'tanh']\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "dropout_rate = None  # No dropout for this test to avoid regularization issues\n",
    "history_logs = []\n",
    "global_suffix_count = 1  # Global suffix count\n",
    "\n",
    "for activation in activation_functions:\n",
    "    for learning_rate in learning_rates:\n",
    "        fold_no = 1\n",
    "        best_val_accuracy = 0\n",
    "        best_epoch = 0\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            model = create_model(activation=activation, learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
    "            history = model.fit(X_train_fold, y_train_fold, epochs=100, batch_size=32, validation_data=(X_val_fold, y_val_fold), verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
    "\n",
    "            for epoch, val_acc in enumerate(history.history['val_accuracy']):\n",
    "                if val_acc > best_val_accuracy:\n",
    "                    best_val_accuracy = val_acc\n",
    "                    best_epoch = epoch + 1\n",
    "\n",
    "            train_accuracies.extend(history.history['accuracy'])\n",
    "            val_accuracies.extend(history.history['val_accuracy'])\n",
    "            train_losses.extend(history.history['loss'])\n",
    "            val_losses.extend(history.history['val_loss'])\n",
    "\n",
    "            fold_no += 1\n",
    "\n",
    "        # Save combined learning and loss curves\n",
    "        plt.figure()\n",
    "        plt.plot(train_accuracies, 'o-', color='r', label='Training accuracy')\n",
    "        plt.plot(val_accuracies, 'o-', color='g', label='Validation accuracy')\n",
    "        plt.title(f'Learning Curve for activation={activation}, learning_rate={learning_rate}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid()\n",
    "        learning_curve_path = os.path.join(graphs_dir, f'learning_curve_{activation}_{learning_rate}_{global_suffix_count}.png')\n",
    "        plt.savefig(learning_curve_path)\n",
    "        plt.close()\n",
    "        global_suffix_count += 1\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses, 'o-', color='r', label='Training loss')\n",
    "        plt.plot(val_losses, 'o-', color='g', label='Validation loss')\n",
    "        plt.title(f'Loss Curve for activation={activation}, learning_rate={learning_rate}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid()\n",
    "        loss_curve_path = os.path.join(graphs_dir, f'loss_curve_{activation}_{learning_rate}_{global_suffix_count}.png')\n",
    "        plt.savefig(loss_curve_path)\n",
    "        plt.close()\n",
    "        global_suffix_count += 1\n",
    "\n",
    "        # Collect history logs\n",
    "        history_logs.append((activation, learning_rate, best_val_accuracy, best_epoch))\n",
    "\n",
    "        # Save model\n",
    "        model_path = os.path.join(models_dir, f'model_{activation}_{learning_rate}.h5')\n",
    "        model.save(model_path)\n",
    "\n",
    "# Validation curves\n",
    "suffix_count = global_suffix_count\n",
    "for activation in activation_functions:\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        model_path = os.path.join(models_dir, f'model_{activation}_{learning_rate}.h5')\n",
    "        model = create_model(activation=activation, learning_rate=learning_rate)\n",
    "        model.load_weights(model_path)\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "        train_scores.append(history.history['accuracy'][-1])\n",
    "        val_scores.append(history.history['val_accuracy'][-1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates, train_scores, 'o-', color='r', label='Training score')\n",
    "    plt.plot(learning_rates, val_scores, 'o-', color='g', label='Validation score')\n",
    "    plt.title(f'Validation Curve for Learning Rate (activation={activation})')\n",
    "    plt.xlabel('Learning rate')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xscale('log')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    validation_curve_path = os.path.join(graphs_dir, f'validation_curve_lr_{activation}_{suffix_count}.png')\n",
    "    plt.savefig(validation_curve_path)\n",
    "    plt.close()\n",
    "\n",
    "    suffix_count += 1\n",
    "\n",
    "# Find the best model\n",
    "best_model_info = max(history_logs, key=lambda x: x[2])\n",
    "best_activation, best_learning_rate, best_val_accuracy, best_epoch = best_model_info\n",
    "\n",
    "best_model_path = os.path.join(models_dir, f'model_{best_activation}_{best_learning_rate}.h5')\n",
    "best_model = create_model(activation=best_activation, learning_rate=best_learning_rate)\n",
    "best_model.load_weights(best_model_path)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Map the numerical labels back to original positions\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_true_labels = label_encoder.inverse_transform(y_true)\n",
    "\n",
    "# Confusion Matrix\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true_labels, y_pred_labels, labels=label_classes)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_classes, yticklabels=label_classes)\n",
    "plt.title('Confusion Matrix for Best Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "conf_matrix_path = os.path.join(graphs_dir, f'confusion_matrix_best_model_{suffix_count}.png')\n",
    "plt.savefig(conf_matrix_path)\n",
    "plt.close()\n",
    "\n",
    "# Mean Squared Error\n",
    "mse = mean_squared_error(y_true, y_pred_classes)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "print(\"All tasks completed successfully. Models and graphs are saved in their respective directories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Position  Count\n",
      "0        CB   3244\n",
      "1        ST   2446\n",
      "2        CM   2189\n",
      "3        GK   2033\n",
      "4       CDM   1511\n",
      "5        RB   1179\n",
      "6        LB   1155\n",
      "7       CAM   1043\n",
      "8        LM    954\n",
      "9        RM    939\n",
      "10       RW    451\n",
      "11       LW    411\n",
      "12      RWB    300\n",
      "13      LWB    265\n",
      "14       CF    130\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'fifa_players_processed.csv'\n",
    "data = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Count the number of each first_position in the dataset\n",
    "position_counts = data['first_position'].value_counts()\n",
    "\n",
    "# Create a DataFrame to display the counts\n",
    "position_counts_df = position_counts.reset_index()\n",
    "position_counts_df.columns = ['Position', 'Count']\n",
    "\n",
    "# Display the DataFrame\n",
    "print(position_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix with percentages saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "# Your confusion matrix (assuming it is stored in conf_matrix)\n",
    "conf_matrix = np.array([\n",
    "    [187, 0, 0, 1, 54, 0, 1, 24, 7, 0, 0, 21, 3, 0, 16],\n",
    "    [0, 835, 50, 0, 1, 0, 14, 0, 0, 0, 22, 0, 0, 0, 0],\n",
    "    [0, 36, 292, 0, 81, 0, 7, 0, 0, 0, 5, 2, 0, 0, 0],\n",
    "    [20, 0, 0, 3, 1, 0, 0, 5, 7, 0, 0, 4, 1, 0, 10],\n",
    "    [57, 2, 66, 0, 513, 0, 6, 7, 1, 0, 6, 16, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 611, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 17, 5, 0, 12, 0, 295, 4, 0, 10, 22, 3, 0, 0, 0],\n",
    "    [43, 0, 2, 0, 13, 0, 14, 98, 23, 3, 3, 53, 11, 0, 33],\n",
    "    [17, 0, 0, 0, 3, 0, 1, 27, 26, 1, 0, 30, 8, 0, 21],\n",
    "    [0, 1, 0, 0, 6, 0, 66, 4, 0, 2, 5, 1, 0, 0, 0],\n",
    "    [0, 25, 14, 0, 13, 0, 2, 2, 0, 0, 289, 11, 0, 5, 0],\n",
    "    [33, 0, 1, 0, 17, 0, 0, 61, 17, 1, 9, 86, 23, 3, 32],\n",
    "    [15, 0, 0, 0, 2, 0, 1, 37, 12, 0, 0, 42, 22, 0, 18],\n",
    "    [1, 1, 3, 0, 5, 0, 1, 1, 0, 1, 72, 12, 0, 2, 0],\n",
    "    [12, 0, 0, 0, 2, 0, 17, 3, 0, 0, 6, 7, 0, 0, 656]\n",
    "])\n",
    "\n",
    "# Class labels (assuming they are stored in label_classes)\n",
    "label_classes = ['CAM', 'CB', 'CDM', 'CF', 'CM', 'GK', 'LB', 'LM', 'LW', 'LWB', 'RB', 'RM', 'RW', 'RWB', 'ST']\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=label_classes, yticklabels=label_classes)\n",
    "plt.title('Confusion Matrix for Best Model (Percentage)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "conf_matrix_path = os.path.join('graphs', f'confusion_matrix_best_model_percentage.png')\n",
    "plt.savefig(conf_matrix_path)\n",
    "plt.close()\n",
    "\n",
    "print(\"Confusion matrix with percentages saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
